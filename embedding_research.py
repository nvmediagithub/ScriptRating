#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ OpenAI –¥–ª—è embeddings.
"""
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EmbeddingResearcher:
    """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ –¥–ª—è embeddings."""
    
    def __init__(self):
        self.free_models = []
        self.openrouter_embedding_models = []
        self.huggingface_models = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/all-MiniLM-L12-v2", 
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "jina-embeddings-v2-base-code",
            "intfloat/multilingual-e5-large",
            "intfloat/e5-large",
            "sentence-transformers/msmarco-distilbert-base-tas-b",
            "sentence-transformers/distiluse-base-multilingual-cased"
        ]
        
    async def get_openrouter_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π OpenRouter."""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    "https://openrouter.ai/api/v1/models",
                    headers={"Authorization": "Bearer demo"}
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("data", [])
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {response.status_code}")
                    return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π OpenRouter: {e}")
            return []
    
    def identify_embedding_models(self, models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å embedding –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–∏ –æ–±—â–µ–≥–æ —Å–ø–∏—Å–∫–∞."""
        embedding_keywords = [
            'embedding', 'embed', 'vector', 'sentence', 'similarity',
            'text-embedding', 'cohere', 'voyage', 'mistral', 'bge', 'e5'
        ]
        
        embedding_models = []
        
        for model in models:
            model_id = model.get("id", "").lower()
            name = model.get("name", "").lower()
            description = model.get("description", "").lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ embedding
            if any(keyword in model_id or keyword in name or keyword in description 
                   for keyword in embedding_keywords):
                embedding_models.append(model)
        
        return embedding_models
    
    def analyze_free_models(self, models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        free_models = []
        
        for model in models:
            pricing = model.get("pricing", {})
            prompt_price = pricing.get("prompt", "0")
            completion_price = pricing.get("completion", "0")
            
            # –°—á–∏—Ç–∞–µ–º –º–æ–¥–µ–ª—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ–π, –µ—Å–ª–∏ —Ü–µ–Ω–∞ 0
            if prompt_price == "0" and completion_price == "0":
                free_models.append({
                    "id": model.get("id"),
                    "name": model.get("name"),
                    "description": model.get("description"),
                    "context_length": model.get("context_length"),
                    "pricing": pricing,
                    "architecture": model.get("architecture", {}),
                    "top_provider": model.get("top_provider", {})
                })
        
        return free_models
    
    async def test_huggingface_api(self, model_name: str) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ HuggingFace."""
        result = {
            "model": model_name,
            "available": False,
            "error": None,
            "embedding_dim": None
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            api_url = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_name}"
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                response = await client.post(
                    api_url,
                    json={"inputs": "test sentence"},
                    headers={"Authorization": f"Bearer {os.getenv('HUGGINGFACE_API_TOKEN', '')}"}
                )
                
                if response.status_code == 200:
                    result["available"] = True
                    result["embedding_dim"] = len(response.json()[0]) if response.json() else None
                else:
                    result["error"] = f"HTTP {response.status_code}: {response.text}"
                    
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    async def research_cohere_embeddings(self) -> Dict[str, Any]:
        """–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Cohere embeddings."""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://api.cohere.ai/v1/embed",
                    headers={
                        "Authorization": f"Bearer {os.getenv('COHERE_API_KEY', '')}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "texts": ["Hello world"],
                        "model": "embed-multilingual-v3.0"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return {
                        "available": True,
                        "model": "embed-multilingual-v3.0",
                        "embedding_dim": len(data.get("embeddings", [{}])[0].get("embedding", [])),
                        "free_tier": True
                    }
                else:
                    return {
                        "available": False,
                        "error": f"HTTP {response.status_code}: {response.text}",
                        "model": "embed-multilingual-v3.0"
                    }
                    
        except Exception as e:
            return {
                "available": False,
                "error": str(e),
                "model": "embed-multilingual-v3.0"
            }
    
    async def test_local_models(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π sentence-transformers."""
        results = {}
        
        for model_name in self.huggingface_models:
            result = {
                "model": model_name,
                "loadable": False,
                "error": None,
                "embedding_dim": None,
                "memory_usage": None
            }
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
                import torch
                from sentence_transformers import SentenceTransformer
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                model = SentenceTransformer(model_name)
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é embedding
                test_embedding = model.encode(["test sentence"])
                
                result["loadable"] = True
                result["embedding_dim"] = test_embedding.shape[1]
                result["device"] = str(next(model.parameters()).device)
                
                # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ (–ø—Ä–∏–º–µ—Ä–Ω–∞—è)
                total_params = sum(p.numel() for p in model.parameters())
                result["memory_usage_mb"] = total_params * 4 / (1024 * 1024)  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                
            except Exception as e:
                result["error"] = str(e)
            
            results[model_name] = result
            
        return results
    
    def generate_report(self, openrouter_models: List[Dict[str, Any]], 
                       free_models: List[Dict[str, Any]], 
                       embedding_models: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏."""
        return {
            "total_openrouter_models": len(openrouter_models),
            "free_openrouter_models": len(free_models),
            "embedding_models_found": len(embedding_models),
            "free_embedding_models": [m for m in free_models if m in embedding_models],
            "recommendations": {
                "best_free_option": "sentence-transformers/all-MiniLM-L6-v2",
                "best_multilingual": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "best_code_model": "jina-embeddings-v2-base-code",
                "best_api_option": "OpenRouter —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏",
                "fallback_strategy": "Local sentence-transformers -> OpenRouter -> Mock"
            },
            "implementation_plan": {
                "1_add_openrouter_embeddings": "–î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É OpenRouter embeddings API",
                "2_fix_sentence_transformers": "–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                "3_add_huggingface_fallback": "–î–æ–±–∞–≤–∏—Ç—å HuggingFace Inference API",
                "4_implement_fallback_chain": "–°–æ–∑–¥–∞—Ç—å —Ü–µ–ø–æ—á–∫—É fallback'–æ–≤",
                "5_test_all_solutions": "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ä–µ—à–µ–Ω–∏—è"
            }
        }


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    researcher = EmbeddingResearcher()
    
    print("üîç –ù–∞—á–∏–Ω–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ –¥–ª—è embeddings...\n")
    
    # 1. –ò—Å—Å–ª–µ–¥—É–µ–º OpenRouter
    print("1Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ OpenRouter –º–æ–¥–µ–ª–µ–π...")
    openrouter_models = await researcher.get_openrouter_models()
    embedding_models = researcher.identify_embedding_models(openrouter_models)
    free_models = researcher.analyze_free_models(openrouter_models)
    
    print(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(openrouter_models)} –º–æ–¥–µ–ª–µ–π –≤ OpenRouter")
    print(f"   üéØ –ù–∞–π–¥–µ–Ω–æ {len(embedding_models)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö embedding –º–æ–¥–µ–ª–µ–π")
    print(f"   üÜì –ù–∞–π–¥–µ–Ω–æ {len(free_models)} –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n")
    
    # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    print("2Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π sentence-transformers...")
    local_results = await researcher.test_local_models()
    
    working_local = [model for model, result in local_results.items() if result["loadable"]]
    print(f"   ‚úÖ –†–∞–±–æ—Ç–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω–æ: {len(working_local)} –º–æ–¥–µ–ª–µ–π")
    for model in working_local:
        result = local_results[model]
        print(f"      - {model}: {result['embedding_dim']}D, ~{result.get('memory_usage_mb', 'N/A')}MB\n")
    
    # 3. –ò—Å—Å–ª–µ–¥—É–µ–º Cohere
    print("3Ô∏è‚É£ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Cohere embeddings...")
    cohere_result = await researcher.research_cohere_embeddings()
    
    if cohere_result["available"]:
        print(f"   ‚úÖ Cohere embed-multilingual-v3.0 –¥–æ—Å—Ç—É–ø–Ω–∞: {cohere_result['embedding_dim']}D")
    else:
        print(f"   ‚ùå Cohree –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {cohere_result.get('error', 'Unknown error')}")
    print()
    
    # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print("4Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
    report = researcher.generate_report(openrouter_models, free_models, embedding_models)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open("embedding_research_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ embedding_research_report.json")
    print("\nüéØ –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print(f"   - –õ—É—á—à–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {report['recommendations']['best_free_option']}")
    print(f"   - –õ—É—á—à–∞—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è: {report['recommendations']['best_multilingual']}")
    print(f"   - –õ—É—á—à–∞—è –¥–ª—è –∫–æ–¥–∞: {report['recommendations']['best_code_model']}")
    print(f"   - –°—Ç—Ä–∞—Ç–µ–≥–∏—è fallback: {report['recommendations']['fallback_strategy']}")
    
    return report


if __name__ == "__main__":
    import os
    asyncio.run(main())