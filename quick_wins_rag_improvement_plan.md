# ScriptRating RAG System - –ü–ª–∞–Ω –±—ã—Å—Ç—Ä—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** 16 –Ω–æ—è–±—Ä—è 2025  
**–§–æ–∫—É—Å:** –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º —ç—Ñ—Ñ–µ–∫—Ç–µ  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** Quick wins –¥–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞

---

## –ê–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º

### –¢–µ–∫—É—â–∏–µ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞:
1. **Mock embeddings** –≤ `app/presentation/api/routes/rag.py:67,161` - –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `[0.1 * i for i in range(384)]`
2. **TF-IDF –ø–æ–∏—Å–∫** –≤ `app/infrastructure/services/knowledge_base.py` - –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ —É—á–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
3. **–ü—Ä–æ—Å—Ç–∞—è chunking** - —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –±–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
4. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ vector database** - –Ω–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
5. **–ë–µ–∑ AI –∞–Ω–∞–ª–∏–∑–∞** - –Ω–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

---

## –¢–û–ü-5 Quick Wins Plan

### 1. üöÄ **–ó–ê–ú–ï–ù–ê MOCK –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ù–ê REAL (1-2 –¥–Ω—è)**
**–°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —à–∞–≥**

#### –ß—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å:
- **–§–∞–π–ª:** `app/presentation/api/routes/rag.py`
- **–°—Ç—Ä–æ–∫–∏:** 67, 161 - –∑–∞–º–µ–Ω–∏—Ç—å `[0.1 * i for i in range(384)]` –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ embeddings
- **–î–æ–±–∞–≤–∏—Ç—å:** `app/domain/services/embedding_service.py` - –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```python
# app/domain/services/embedding_service.py
from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingService:
    def __init__(self):
        # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è ScriptRating
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    async def generate_embeddings(self, texts: list) -> list:
        return self.model.encode(texts).tolist()
```

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ RAG routes:
```python
# app/presentation/api/routes/rag.py
from app.domain.services.embedding_service import EmbeddingService

embedding_service = EmbeddingService()

def _generate_mock_corpus():
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    embeddings = await embedding_service.generate_embeddings([content])
    item["embedding"] = embeddings[0]  # –†–µ–∞–ª—å–Ω—ã–µ embeddings –≤–º–µ—Å—Ç–æ mock
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞:** +35-50%
- **–¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 4-6 —á–∞—Å–æ–≤ (1 —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, 1 –¥–µ–Ω—å)
- **–†–∏—Å–∫:** –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:** –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–º–µ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã—Ö

#### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- `pip install sentence-transformers`
- PyTorch –¥–ª—è –º–æ–¥–µ–ª–∏

---

### 2. üîÑ **–ò–ù–¢–ï–ì–†–ê–¶–ò–Ø VECTOR DATABASE (3-5 –¥–Ω–µ–π)**
**–ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç TF-IDF –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É vector search**

#### –ß—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å:
- **–§–∞–π–ª:** `app/infrastructure/services/knowledge_base.py` - –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞
- **–î–æ–±–∞–≤–∏—Ç—å:** `app/infrastructure/services/vector_database_service.py`
- **–§–∞–π–ª:** `docker-compose.yml` - –¥–æ–±–∞–≤–∏—Ç—å Qdrant

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```yaml
# docker-compose.yml
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage

volumes:
  qdrant_storage:
```

```python
# app/infrastructure/services/vector_database_service.py
from qdrant_client import QdrantClient
from qdrant_client.http import models
import asyncio

class VectorDatabaseService:
    def __init__(self):
        self.client = QdrantClient(host="localhost", port=6333)
        self.collection_name = "scriptrating_knowledge"
        self._ensure_collection()
    
    def _ensure_collection(self):
        collections = self.client.get_collections()
        if self.collection_name not in [c.name for c in collections.collections]:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=384,  # —Ä–∞–∑–º–µ—Ä embeddings
                    distance=models.Distance.COSINE
                )
            )
```

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ KnowledgeBase:
```python
# app/infrastructure/services/knowledge_base.py
class KnowledgeBase:
    def __init__(self):
        self._entries: List[KnowledgeEntry] = []
        self.vector_db = VectorDatabaseService()  # –ù–û–í–û–ï
        self.embedding_service = EmbeddingService()  # –ù–û–í–û–ï
    
    async def query(self, text: str, top_k: int = 3) -> List[Dict[str, Any]]:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = await self.embedding_service.generate_embeddings([text])
        
        # Vector search —á–µ—Ä–µ–∑ Qdrant
        results = await self.vector_db.similarity_search(
            query_vector=query_embedding[0],
            top_k=top_k
        )
        
        return results
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞:** +40-60%
- **–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞:** < 200ms –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
- **–¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 12-16 —á–∞—Å–æ–≤ (1 —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, 2 –¥–Ω—è)
- **–†–∏—Å–∫:** –°—Ä–µ–¥–Ω–∏–π (—Ç—Ä–µ–±—É–µ—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö)

#### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- Docker –¥–ª—è Qdrant
- `pip install qdrant-client`

---

### 3. üìù **–£–õ–£–ß–®–ï–ù–ù–ê–Ø CHUNKING –°–¢–†–ê–¢–ï–ì–ò–Ø (1 –Ω–µ–¥–µ–ª—è)**
**Semantic-aware —Ä–∞–∑–±–∏–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ AI**

#### –ß—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å:
- **–§–∞–π–ª:** `app/domain/services/document_chunking_service.py` - –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å
- **–§–∞–π–ª:** `app/infrastructure/services/knowledge_base.py` - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–π chunking —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```python
# app/domain/services/document_chunking_service.py
import re
from typing import List, Dict, Any

class DocumentChunkingService:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    
    def chunk_document(self, text: str, doc_type: str = "script") -> List[Dict[str, Any]]:
        if doc_type == "script":
            return self._chunk_script(text)
        else:
            return self._chunk_legal_document(text)
    
    def _chunk_script(self, text: str) -> List[Dict[str, Any]]:
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å—Ü–µ–Ω–∞–º (INT./EXT. LOCATION - TIME)
        scene_pattern = r'(INT\.|EXT\.|INT/EXT\.)[^\n]*\n+'
        scenes = re.split(scene_pattern, text)
        
        for i, scene in enumerate(scenes[1:], 1):  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
            if len(scene.strip()) < 50:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ü–µ–Ω—ã
                continue
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ü–µ–Ω—ã
            lines = scene.split('\n')
            scene_header = lines[0] if lines else ""
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è
            dialogue_chunks = self._group_dialogues(lines[1:] if lines else [])
            
            for chunk in dialogue_chunks:
                chunks.append({
                    "text": chunk,
                    "chunk_type": "scene" if "INT." in scene_header or "EXT." in scene_header else "dialogue",
                    "scene_number": i,
                    "context": scene_header[:100]  # –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ü–µ–Ω—ã
                })
        
        return chunks
    
    def _chunk_legal_document(self, text: str) -> List[Dict[str, Any]]:
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å—Ç–∞—Ç—å—è–º –∏ –ø—É–Ω–∫—Ç–∞–º
        article_pattern = r'(–°—Ç–∞—Ç—å—è\s+\d+[–∞-—è]?\.?\s*[^.]*\.)\s*'
        articles = re.split(article_pattern, text)
        
        for i in range(0, len(articles)-1, 2):
            if i+1 < len(articles):
                article_title = articles[i]
                article_content = articles[i+1]
                
                # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –ø—É–Ω–∫—Ç—ã
                points = re.split(r'(\d+\.)\s+', article_content)
                
                for j in range(0, len(points)-1, 2):
                    if j+1 < len(points):
                        point_number = points[j]
                        point_content = points[j+1]
                        
                        chunks.append({
                            "text": f"{article_title} {point_number} {point_content}",
                            "chunk_type": "legal_article",
                            "article": article_title[:100],
                            "point": point_number
                        })
        
        return chunks
    
    def _group_dialogues(self, lines: List[str]) -> List[str]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º"""
        chunks = []
        current_chunk = []
        current_character = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ (CAPS –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏)
            character_match = re.match(r'^[A-Z–ê-–Ø\s]+$', line)
            if character_match and len(line) < 50:
                # –ù–æ–≤—ã–π –ø–µ—Ä—Å–æ–Ω–∞–∂ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_character = line
            else:
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                current_chunk.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
```

#### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ KnowledgeBase:
```python
# app/infrastructure/services/knowledge_base.py
from app.domain.services.document_chunking_service import DocumentChunkingService

class KnowledgeBase:
    def __init__(self):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        self.chunking_service = DocumentChunkingService()  # –ù–û–í–û–ï
    
    async def ingest_document(self, document_id: str, document_title: str, 
                            paragraph_details: List[Dict[str, Any]], doc_type: str = "script"):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é chunking —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        full_text = "\n".join([detail.get("text", "") for detail in paragraph_details])
        enhanced_chunks = self.chunking_service.chunk_document(full_text, doc_type)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ KnowledgeEntry
        cleaned_entries = [
            KnowledgeEntry(
                entry_id=str(uuid.uuid4()),
                document_id=document_id,
                document_title=document_title,
                page=int(detail.get("page", 1)),
                paragraph=int(detail.get("paragraph_index", 1)),
                text=chunk["text"],
                metadata={
                    **detail,
                    "chunk_type": chunk.get("chunk_type", "paragraph"),
                    "enhanced_chunking": True
                },
            )
            for chunk in enhanced_chunks
            for detail in paragraph_details[:1]  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if chunk.get("text", "").strip()
        ]
        
        # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ...
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:** +30-40%
- **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:** –°—Ü–µ–Ω—ã –∏ –¥–∏–∞–ª–æ–≥–∏ –æ—Å—Ç–∞—é—Ç—Å—è —Ü–µ–ª—ã–º–∏
- **–¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 24-30 —á–∞—Å–æ–≤ (1 —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, 1 –Ω–µ–¥–µ–ª—è)
- **–†–∏—Å–∫:** –ù–∏–∑–∫–∏–π (–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ª–æ–≥–∏–∫–µ)

---

### 4. ‚ö° **–ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (1-2 –Ω–µ–¥–µ–ª–∏)**
**Performance optimization –∏ batch processing**

#### –ß—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å:
- **–§–∞–π–ª:** `app/infrastructure/services/cache_service.py` - –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å
- **–§–∞–π–ª:** `app/domain/services/embedding_service.py` - batch processing
- **–§–∞–π–ª:** `docker-compose.yml` - –¥–æ–±–∞–≤–∏—Ç—å Redis

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```yaml
# docker-compose.yml
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

```python
# app/infrastructure/services/cache_service.py
import redis.asyncio as redis
import json
import hashlib
from typing import Any, Optional

class CacheService:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.default_ttl = 3600 * 24  # 24 —á–∞—Å–∞
    
    def _generate_key(self, prefix: str, content: str) -> str:
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"{prefix}:{content_hash}"
    
    async def get_embeddings(self, text: str) -> Optional[List[float]]:
        key = self._generate_key("embedding", text)
        cached = await self.redis_client.get(key)
        if cached:
            return json.loads(cached)
        return None
    
    async def set_embeddings(self, text: str, embeddings: List[float], ttl: int = None):
        key = self._generate_key("embedding", text)
        await self.redis_client.setex(
            key, 
            ttl or self.default_ttl, 
            json.dumps(embeddings)
        )
```

```python
# app/domain/services/embedding_service.py - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
import asyncio
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from app.infrastructure.services.cache_service import CacheService

class EmbeddingService:
    def __init__(self):
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.cache = CacheService()
        self.batch_size = 32
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Batch processing —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        results = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            cached_embedding = await self.cache.get_embeddings(text)
            if cached_embedding:
                results.append(cached_embedding)
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –Ω–µ–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        if uncached_texts:
            for i in range(0, len(uncached_texts), self.batch_size):
                batch = uncached_texts[i:i + self.batch_size]
                embeddings = self.model.encode(batch).tolist()
                
                # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for j, embedding in enumerate(embeddings):
                    text = batch[j]
                    await self.cache.set_embeddings(text, embedding)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for j, embedding in enumerate(embeddings):
                    original_index = uncached_indices[i + j]
                    results.insert(original_index, embedding)
        
        return results
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **–°–∫–æ—Ä–æ—Å—Ç—å:** +60-80% –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- **–ù–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ API:** -70% –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings
- **–¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 30-40 —á–∞—Å–æ–≤ (1 —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, 1.5 –Ω–µ–¥–µ–ª–∏)
- **–†–∏—Å–∫:** –ù–∏–∑–∫–∏–π (—Ç–æ–ª—å–∫–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)

---

### 5. ü§ñ **AI ANALYTICS –ò –ú–û–ù–ò–¢–û–†–ò–ù–ì (2-3 –Ω–µ–¥–µ–ª–∏)**
**–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞**

#### –ß—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—å:
- **–§–∞–π–ª:** `app/domain/services/ai_analytics_service.py` - –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å
- **–§–∞–π–ª:** `app/presentation/api/routes/rag.py` - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ analytics
- **–î–æ–±–∞–≤–∏—Ç—å:** monitoring –∏ quality metrics

#### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```python
# app/domain/services/ai_analytics_service.py
from typing import List, Dict, Any
import asyncio
from openai import AsyncOpenAI

class AIAnalyticsService:
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ RAG —Å –ø–æ–º–æ—â—å—é –≥–æ—Ç–æ–≤—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤"""
    
    def __init__(self):
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    async def analyze_query_relevance(self, query: str, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º prompt –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        prompt = f"""
        –û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞:
        
        –ó–∞–ø—Ä–æ—Å: "{query}"
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
        {chr(10).join([f"{i+1}. {result.get('content', '')[:200]}..." for i, result in enumerate(results)])}
        
        –û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ —à–∫–∞–ª–µ –æ—Ç 1 –¥–æ 5:
        - 1: –°–æ–≤—Å–µ–º –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
        - 2: –°–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
        - 3: –£–º–µ—Ä–µ–Ω–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
        - 4: –†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
        - 5: –û—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
        
        –í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {{"scores": [1, 2, 3, ...], "average_score": 2.5, "overall_quality": "good"}}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            content = response.choices[0].message.content
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å proper JSON parsing
            return {"quality_score": 3.5, "analysis": content}
            
        except Exception as e:
            return {"quality_score": 3.0, "error": str(e)}
    
    async def suggest_query_improvements(self, original_query: str, results: List[Dict[str, Any]]) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∑–∞–ø—Ä–æ—Å–∞"""
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è:
        
        –ó–∞–ø—Ä–æ—Å: "{original_query}"
        –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(results)}
        
        –ü—Ä–µ–¥–ª–æ–∂–∏ 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ —É–ª—É—á—à–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            
            suggestions = response.choices[0].message.content.split('\n')
            return [s.strip('- ') for s in suggestions if s.strip()]
            
        except Exception as e:
            return ["–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤", "–£—Ç–æ—á–Ω–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞"]
```

#### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ RAG routes:
```python
# app/presentation/api/routes/rag.py
from app.domain.services.ai_analytics_service import AIAnalyticsService

analytics_service = AIAnalyticsService()

@router.post("/query", response_model=RAGQueryResponse)
async def query_rag(request: RAGQueryRequest) -> RAGQueryResponse:
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    # –î–æ–±–∞–≤–ª—è–µ–º AI –∞–Ω–∞–ª–∏—Ç–∏–∫—É
    if request.include_analytics:
        relevance_analysis = await analytics_service.analyze_query_relevance(request.query, results)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º response —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π
        return RAGQueryResponse(
            query=request.query,
            results=results,
            total_found=len(relevant_docs),
            analytics={
                "relevance_score": relevance_analysis["quality_score"],
                "suggestions": await analytics_service.suggest_query_improvements(request.query, results)
            }
        )
    
    return RAGQueryResponse(query=request.query, results=results, total_found=len(relevant_docs))
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç:** +25-35% —á–µ—Ä–µ–∑ suggestions
- **–ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞:** +15-20% —á–µ—Ä–µ–∑ feedback
- **–¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 40-60 —á–∞—Å–æ–≤ (1 —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, 2-3 –Ω–µ–¥–µ–ª–∏)
- **–†–∏—Å–∫:** –°—Ä–µ–¥–Ω–∏–π (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö API)

---

## –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ Quick Wins

### –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:
| –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –®–∞–≥ | –¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã | –£–ª—É—á—à–µ–Ω–∏–µ | –í—Ä–µ–º—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è |
|-----------|-----|--------------|-----------|-----------------|
| 1 | Real Embeddings | 4-6 —á–∞—Å–æ–≤ | +35-50% | 1-2 –¥–Ω—è |
| 2 | Vector Database | 12-16 —á–∞—Å–æ–≤ | +40-60% | 3-5 –¥–Ω–µ–π |
| 3 | Enhanced Chunking | 24-30 —á–∞—Å–æ–≤ | +30-40% | 1 –Ω–µ–¥–µ–ª—è |
| 4 | Caching & Optimization | 30-40 —á–∞—Å–æ–≤ | +60-80% | 1-2 –Ω–µ–¥–µ–ª–∏ |
| 5 | AI Analytics | 40-60 —á–∞—Å–æ–≤ | +25-35% | 2-3 –Ω–µ–¥–µ–ª–∏ |

### –û–±—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç –æ—Ç –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π:
- **–ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞:** +80-120%
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** +70-100%
- **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç:** +90-110%
- **–û–±—â–µ–µ –≤—Ä–µ–º—è:** 6-8 –Ω–µ–¥–µ–ª—å
- **–û–±—â–∏–µ —Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã:** 110-152 —á–∞—Å–∞

---

## –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

### –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∏—Å–∫–æ–≤:
1. **External API dependencies** (—à–∞–≥ 5)
   - **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** Rate limiting, fallback strategies, local alternatives
2. **Data migration** (—à–∞–≥ 2)
   - **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** Backup procedures, gradual migration, rollback plan
3. **Performance degradation** (—à–∞–≥ 2)
   - **–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** Load testing, monitoring, auto-scaling

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:
- [ ] Search relevance improvement: > 40%
- [ ] Response time: < 500ms
- [ ] User satisfaction: > 85%
- [ ] System availability: > 99%

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (—Å–ª–µ–¥—É—é—â–∞—è –Ω–µ–¥–µ–ª—è):
1. **–î–µ–Ω—å 1-2**: Implement real embeddings (—à–∞–≥ 1)
2. **–î–µ–Ω—å 3-5**: Setup Qdrant –∏ implement vector search (—à–∞–≥ 2)
3. **–ù–µ–¥–µ–ª—è 2**: Enhanced chunking strategy (—à–∞–≥ 3)

### –ü–ª–∞–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:
- –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π monitoring –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
- Weekly performance reviews
- Monthly user feedback analysis

**–î–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –≥–æ—Ç–æ–≤ –∫ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–º—É –≤–Ω–µ–¥—Ä–µ–Ω–∏—é —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –±—ã—Å—Ç—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.**