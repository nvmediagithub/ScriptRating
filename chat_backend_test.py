#!/usr/bin/env python3
"""
Backend Chat Functionality Test Script

This script tests the newly implemented backend chat functionality
without requiring the full application to start.
"""
import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional
from uuid import uuid4

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import chat models and schemas directly
try:
    from app.models.chat_models import ChatSession, ChatMessage, create_chat_session
    from app.presentation.api.schemas.chat_schemas import (
        ChatSessionCreateRequest, ChatSessionResponse, ChatMessageCreateRequest,
        MessageRole, LLMProvider, format_chat_session, format_chat_message
    )
    print("‚úÖ Successfully imported chat models and schemas")
except ImportError as e:
    print(f"‚ùå Failed to import chat models: {e}")
    exit(1)

class ChatAPITester:
    """Test harness for chat API functionality."""
    
    def __init__(self):
        self.test_results = {}
        self.mock_db = {}  # Simple in-memory storage for testing
        
    def log_test(self, test_name: str, success: bool, message: str = "", details: Any = None):
        """Log test results."""
        result = {
            "success": success,
            "message": message,
            "timestamp": datetime.utcnow().isoformat(),
            "details": details
        }
        self.test_results[test_name] = result
        
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}: {message}")
        if details:
            print(f"   Details: {details}")
    
    def test_chat_models_creation(self):
        """Test chat models can be created and manipulated."""
        test_name = "chat_models_creation"
        
        try:
            # Test ChatSession creation (mock)
            session_data = {
                "id": str(uuid4()),
                "user_id": "test_user_123",
                "title": "Test Chat Session",
                "llm_provider": "LOCAL",
                "llm_model": "llama2:7b",
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
                "is_active": True,
                "settings": {}
            }
            
            # Test model instantiation
            session = ChatSession(**session_data)
            
            # Test to_dict method
            session_dict = session.to_dict()
            
            # Validate key fields
            required_fields = ["id", "user_id", "title", "llm_provider", "llm_model", "is_active"]
            missing_fields = [field for field in required_fields if field not in session_dict]
            
            if missing_fields:
                self.log_test(test_name, False, f"Missing required fields: {missing_fields}")
                return
            
            # Test ChatMessage creation (mock)
            message_data = {
                "id": str(uuid4()),
                "session_id": session.id,
                "role": "user",
                "content": "Hello, this is a test message",
                "created_at": datetime.utcnow(),
                "is_streaming": False,
                "tokens_used": 0,
                "message_metadata": {}
            }
            
            message = ChatMessage(**message_data)
            message_dict = message.to_dict()
            
            required_message_fields = ["id", "session_id", "role", "content"]
            missing_message_fields = [field for field in required_message_fields if field not in message_dict]
            
            if missing_message_fields:
                self.log_test(test_name, False, f"Missing required message fields: {missing_message_fields}")
                return
            
            self.log_test(test_name, True, "Chat models created successfully", {
                "session_id": session.id,
                "message_id": message.id
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"Error creating chat models: {str(e)}")
    
    def test_chat_schemas(self):
        """Test chat API schemas validation."""
        test_name = "chat_schemas_validation"
        
        try:
            # Test ChatSessionCreateRequest schema
            session_request_data = {
                "title": "Test Session Schema",
                "llm_provider": LLMProvider.LOCAL,
                "llm_model": "llama2:7b",
                "settings": {"temperature": 0.7}
            }
            
            session_request = ChatSessionCreateRequest(**session_request_data)
            
            # Test schema fields
            if session_request.title != "Test Session Schema":
                self.log_test(test_name, False, "Title field validation failed")
                return
            
            if session_request.llm_provider != LLMProvider.LOCAL:
                self.log_test(test_name, False, "LLM provider validation failed")
                return
            
            # Test ChatMessageCreateRequest schema
            message_request_data = {
                "content": "This is a test message for schema validation",
                "role": MessageRole.USER,
                "metadata": {"test": True}
            }
            
            message_request = ChatMessageCreateRequest(**message_request_data)
            
            if message_request.content != "This is a test message for schema validation":
                self.log_test(test_name, False, "Message content validation failed")
                return
            
            if message_request.role != MessageRole.USER:
                self.log_test(test_name, False, "Message role validation failed")
                return
            
            # Test enum values
            llm_providers = [provider.value for provider in LLMProvider]
            message_roles = [role.value for role in MessageRole]
            
            if "LOCAL" not in llm_providers or "OPENROUTER" not in llm_providers:
                self.log_test(test_name, False, "LLMProvider enum values missing")
                return
            
            if "user" not in message_roles or "assistant" not in message_roles:
                self.log_test(test_name, False, "MessageRole enum values missing")
                return
            
            self.log_test(test_name, True, "All schema validations passed", {
                "llm_providers": llm_providers,
                "message_roles": message_roles
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"Schema validation error: {str(e)}")
    
    def test_formatting_functions(self):
        """Test formatting functions for API responses."""
        test_name = "chat_formatting_functions"
        
        try:
            # Create mock session and message
            session = ChatSession(
                id="test_session_123",
                user_id="test_user",
                title="Test Formatting",
                llm_provider="LOCAL",
                llm_model="llama2:7b",
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_active=True,
                settings={}
            )
            
            # Test format_chat_session
            formatted_session = format_chat_session(session)
            
            if not isinstance(formatted_session, dict):
                self.log_test(test_name, False, "format_chat_session should return dict")
                return
            
            # Check required fields
            required_session_fields = ["id", "title", "user_id", "llm_provider", "is_active"]
            for field in required_session_fields:
                if field not in formatted_session:
                    self.log_test(test_name, False, f"Missing field in formatted session: {field}")
                    return
            
            # Create mock message
            message = ChatMessage(
                id="test_message_456",
                session_id="test_session_123",
                role="user",
                content="Test message for formatting",
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_streaming=False,
                tokens_used=5,
                message_metadata={}
            )
            
            # Test format_chat_message
            formatted_message = format_chat_message(message)
            
            if not isinstance(formatted_message, dict):
                self.log_test(test_name, False, "format_chat_message should return dict")
                return
            
            # Check required fields
            required_message_fields = ["id", "session_id", "role", "content", "is_streaming"]
            for field in required_message_fields:
                if field not in formatted_message:
                    self.log_test(test_name, False, f"Missing field in formatted message: {field}")
                    return
            
            self.log_test(test_name, True, "Formatting functions work correctly", {
                "session_fields": len(formatted_session),
                "message_fields": len(formatted_message)
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"Formatting function error: {str(e)}")
    
    def test_chat_constants(self):
        """Test chat-related constants and enums."""
        test_name = "chat_constants_validation"
        
        try:
            # Test LLMProvider enum
            expected_providers = ["LOCAL", "OPENROUTER"]
            actual_providers = [provider.value for provider in LLMProvider]
            
            for provider in expected_providers:
                if provider not in actual_providers:
                    self.log_test(test_name, False, f"Missing LLMProvider: {provider}")
                    return
            
            # Test MessageRole enum
            expected_roles = ["user", "assistant", "system"]
            actual_roles = [role.value for role in MessageRole]
            
            for role in expected_roles:
                if role not in actual_roles:
                    self.log_test(test_name, False, f"Missing MessageRole: {role}")
                    return
            
            # Test enum ordering
            providers_list = list(LLMProvider)
            roles_list = list(MessageRole)
            
            if len(providers_list) != 2:
                self.log_test(test_name, False, f"Expected 2 LLM providers, got {len(providers_list)}")
                return
            
            if len(roles_list) != 3:
                self.log_test(test_name, False, f"Expected 3 message roles, got {len(roles_list)}")
                return
            
            self.log_test(test_name, True, "All chat constants validated", {
                "providers": actual_providers,
                "roles": actual_roles
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"Constants validation error: {str(e)}")
    
    def test_database_integration_simulation(self):
        """Test database integration logic (simulated)."""
        test_name = "database_integration_simulation"
        
        try:
            # Simulate database operations
            session_id = str(uuid4())
            user_id = "test_user_simulation"
            
            # Test session creation simulation
            new_session = ChatSession(
                id=session_id,
                user_id=user_id,
                title="Database Test Session",
                llm_provider="LOCAL",
                llm_model="llama2:7b",
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_active=True,
                settings={}
            )
            
            # Store in mock database
            self.mock_db[f"session_{session_id}"] = new_session
            
            # Test message creation simulation
            message_id = str(uuid4())
            new_message = ChatMessage(
                id=message_id,
                session_id=session_id,
                role="user",
                content="Database integration test message",
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_streaming=False,
                tokens_used=0,
                message_metadata={}
            )
            
            # Store in mock database
            self.mock_db[f"message_{message_id}"] = new_message
            
            # Test retrieval
            retrieved_session = self.mock_db.get(f"session_{session_id}")
            retrieved_message = self.mock_db.get(f"message_{message_id}")
            
            if not retrieved_session:
                self.log_test(test_name, False, "Failed to retrieve session from mock database")
                return
            
            if not retrieved_message:
                self.log_test(test_name, False, "Failed to retrieve message from mock database")
                return
            
            # Test relationship (mock)
            if str(retrieved_message.session_id) != retrieved_session.id:
                self.log_test(test_name, False, "Message-session relationship broken")
                return
            
            # Test session message count
            session_messages = [
                msg for msg in self.mock_db.values() 
                if hasattr(msg, 'session_id') and str(msg.session_id) == session_id
            ]
            
            if len(session_messages) != 1:
                self.log_test(test_name, False, f"Expected 1 message for session, got {len(session_messages)}")
                return
            
            self.log_test(test_name, True, "Database integration simulation successful", {
                "session_id": session_id,
                "message_id": message_id,
                "total_stored": len(self.mock_db)
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"Database integration simulation error: {str(e)}")
    
    def test_api_endpoint_simulation(self):
        """Test API endpoint logic simulation."""
        test_name = "api_endpoint_simulation"
        
        try:
            # Simulate API endpoint logic from chat.py
            
            # 1. Test session creation endpoint logic
            session_request = ChatSessionCreateRequest(
                title="API Test Session",
                llm_provider=LLMProvider.LOCAL,
                llm_model="llama2:7b",
                settings={"temperature": 0.8}
            )
            
            # Simulate session creation
            session = ChatSession(
                id=str(uuid4()),
                user_id="api_test_user",
                title=session_request.title,
                llm_provider=session_request.llm_provider,
                llm_model=session_request.llm_model,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_active=True,
                settings=session_request.settings or {}
            )
            
            formatted_session = format_chat_session(session)
            
            # 2. Test message creation endpoint logic
            message_request = ChatMessageCreateRequest(
                content="This is a test message for API simulation",
                role=MessageRole.USER,
                metadata={"test": True}
            )
            
            message = ChatMessage(
                id=str(uuid4()),
                session_id=session.id,
                role=message_request.role,
                content=message_request.content,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
                is_streaming=False,
                tokens_used=0,
                message_metadata=message_request.metadata or {}
            )
            
            formatted_message = format_chat_message(message)
            
            # 3. Test pagination logic
            mock_sessions = [session]  # In real case, this would be from database
            page = 1
            page_size = 10
            
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            paginated_sessions = mock_sessions[start_idx:end_idx]
            
            # 4. Test statistics simulation
            stats = {
                "total_sessions": 1,
                "total_messages": 1,
                "active_sessions": 1,
                "messages_today": 1,
                "average_response_time_ms": 1000.0,
                "most_used_provider": "LOCAL",
                "most_used_model": "llama2:7b"
            }
            
            # Validate responses
            if not formatted_session.get("id"):
                self.log_test(test_name, False, "Session response missing ID")
                return
            
            if not formatted_message.get("id"):
                self.log_test(test_name, False, "Message response missing ID")
                return
            
            if len(paginated_sessions) != 1:
                self.log_test(test_name, False, "Pagination not working correctly")
                return
            
            required_stats_fields = ["total_sessions", "total_messages", "active_sessions"]
            for field in required_stats_fields:
                if field not in stats:
                    self.log_test(test_name, False, f"Statistics missing field: {field}")
                    return
            
            self.log_test(test_name, True, "API endpoint simulation successful", {
                "session_id": formatted_session["id"],
                "message_id": formatted_message["id"],
                "paginated_count": len(paginated_sessions),
                "stats_fields": len(stats)
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"API endpoint simulation error: {str(e)}")
    
    def test_websocket_simulation(self):
        """Test WebSocket functionality simulation."""
        test_name = "websocket_simulation"
        
        try:
            # Simulate WebSocket message types from schemas
            message_types = [
                "connection_established",
                "user_joined", 
                "user_left",
                "message_update",
                "typing_indicator",
                "error",
                "ping",
                "pong",
                "message_ack"
            ]
            
            # Simulate WebSocket connection management
            active_connections = {}
            session_users = {}
            
            # Simulate user connection
            session_id = "test_session_ws"
            user_id = "test_user_ws"
            
            # Simulate connection establishment
            connection_id = str(uuid4())
            active_connections[session_id] = {connection_id}
            session_users[session_id] = {user_id: connection_id}
            
            # Simulate message broadcasting
            broadcast_message = {
                "type": "message_update",
                "session_id": session_id,
                "user_id": user_id,
                "data": {
                    "message_id": str(uuid4()),
                    "content": "WebSocket test message",
                    "timestamp": datetime.utcnow().isoformat()
                }
            }
            
            # Simulate message handling
            if broadcast_message["type"] not in message_types:
                self.log_test(test_name, False, f"Invalid message type: {broadcast_message['type']}")
                return
            
            # Simulate typing indicator
            typing_message = {
                "type": "typing_indicator",
                "session_id": session_id,
                "user_id": user_id,
                "data": {
                    "is_typing": True
                }
            }
            
            # Simulate connection cleanup
            connection_id_cleanup = session_users[session_id].get(user_id)
            if connection_id_cleanup:
                active_connections[session_id].discard(connection_id_cleanup)
                del session_users[session_id][user_id]
                if not active_connections[session_id]:
                    del active_connections[session_id]
                    if session_id in session_users:
                        del session_users[session_id]
            
            self.log_test(test_name, True, "WebSocket simulation successful", {
                "message_types": len(message_types),
                "connection_tracked": len(active_connections),
                "users_tracked": len(session_users)
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"WebSocket simulation error: {str(e)}")
    
    def test_llm_integration_simulation(self):
        """Test LLM integration logic simulation."""
        test_name = "llm_integration_simulation"
        
        try:
            # Simulate LLM processing from chat.py
            
            # Mock LLM request
            class MockLLMRequest:
                def __init__(self, prompt, provider, model_name):
                    self.prompt = prompt
                    self.provider = provider
                    self.model_name = model_name
            
            # Mock LLM response
            class MockLLMResponse:
                def __init__(self, response, tokens_used, response_time_ms, provider, model):
                    self.response = response
                    self.tokens_used = tokens_used
                    self.response_time_ms = response_time_ms
                    self.provider = provider
                    self.model = model
                    self.success = True
            
            # Test LLM provider simulation
            providers = [LLMProvider.LOCAL, LLMProvider.OPENROUTER]
            
            for provider in providers:
                # Simulate different responses based on provider
                if provider == LLMProvider.LOCAL:
                    mock_response = "I understand your question. Let me help you with that."
                    model_name = "llama2:7b"
                else:  # OpenRouter
                    mock_response = "Thank you for your question. Based on my analysis, I can provide insights about this topic."
                    model_name = "gpt-3.5-turbo"
                
                # Simulate LLM processing
                request = MockLLMRequest(
                    prompt="What is the meaning of life?",
                    provider=provider,
                    model_name=model_name
                )
                
                # Simulate processing time
                processing_start = datetime.utcnow()
                
                # Mock response
                response = MockLLMResponse(
                    response=mock_response,
                    tokens_used=len(mock_response.split()) * 1.3,  # Rough estimation
                    response_time_ms=(datetime.utcnow() - processing_start).total_seconds() * 1000,
                    provider=request.provider,
                    model=request.model_name
                )
                
                # Validate response
                if not response.response:
                    self.log_test(test_name, False, f"Empty response from {provider}")
                    return
                
                if response.tokens_used <= 0:
                    self.log_test(test_name, False, f"Invalid token count from {provider}")
                    return
                
                if response.response_time_ms < 0:
                    self.log_test(test_name, False, f"Invalid response time from {provider}")
                    return
                
                if not response.success:
                    self.log_test(test_name, False, f"LLM processing failed for {provider}")
                    return
            
            # Test error handling
            error_request = MockLLMRequest(
                prompt="Test error scenario",
                provider=LLMProvider.LOCAL,
                model_name="invalid_model"
            )
            
            # Simulate error response
            error_response = MockLLMResponse(
                response="",
                tokens_used=0,
                response_time_ms=0,
                provider=error_request.provider,
                model=error_request.model_name
            )
            error_response.success = False
            error_response.error_message = "Model not found"
            
            if error_response.success:
                self.log_test(test_name, False, "Error simulation failed - should have been unsuccessful")
                return
            
            self.log_test(test_name, True, "LLM integration simulation successful", {
                "providers_tested": len(providers),
                "error_handling": "simulated"
            })
            
        except Exception as e:
            self.log_test(test_name, False, f"LLM integration simulation error: {str(e)}")
    
    def generate_test_report(self):
        """Generate comprehensive test report."""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result["success"])
        failed_tests = total_tests - passed_tests
        
        print("\n" + "="*80)
        print("BACKEND CHAT FUNCTIONALITY TEST REPORT")
        print("="*80)
        print(f"Test Execution Time: {datetime.utcnow().isoformat()}")
        print(f"Total Tests: {total_tests}")
        print(f"Passed: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
        print(f"Failed: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        print("\nDETAILED RESULTS:")
        print("-"*80)
        
        for test_name, result in self.test_results.items():
            status = "‚úÖ PASS" if result["success"] else "‚ùå FAIL"
            print(f"\n{status} {test_name}")
            print(f"   Message: {result['message']}")
            if result["details"]:
                print(f"   Details: {json.dumps(result['details'], indent=6, default=str)}")
        
        print("\n" + "="*80)
        print("BACKEND API ENDPOINTS TESTED:")
        print("-"*80)
        
        endpoint_tests = {
            "Session Management": ["chat_models_creation", "chat_schemas_validation"],
            "Message Handling": ["chat_formatting_functions", "database_integration_simulation"],
            "API Integration": ["api_endpoint_simulation", "chat_constants_validation"],
            "Real-time Features": ["websocket_simulation"],
            "LLM Integration": ["llm_integration_simulation"]
        }
        
        for category, tests in endpoint_tests.items():
            category_passed = sum(1 for test in tests if self.test_results.get(test, {}).get("success", False))
            category_total = len(tests)
            print(f"{category}: {category_passed}/{category_total} tests passed")
        
        print("\n" + "="*80)
        
        if failed_tests == 0:
            print("üéâ ALL TESTS PASSED! Backend chat functionality is working correctly.")
        else:
            print(f"‚ö†Ô∏è  {failed_tests} test(s) failed. Review the issues above.")
        
        print("="*80)
        
        return {
            "total_tests": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "success_rate": passed_tests/total_tests*100,
            "results": self.test_results
        }

def main():
    """Run comprehensive backend chat functionality tests."""
    print("Starting Backend Chat Functionality Tests...")
    print("="*60)
    
    tester = ChatAPITester()
    
    # Run all tests
    tester.test_chat_models_creation()
    tester.test_chat_schemas()
    tester.test_formatting_functions()
    tester.test_chat_constants()
    tester.test_database_integration_simulation()
    tester.test_api_endpoint_simulation()
    tester.test_websocket_simulation()
    tester.test_llm_integration_simulation()
    
    # Generate comprehensive report
    report = tester.generate_test_report()
    
    return report

if __name__ == "__main__":
    main()