# Техническое предложение: Улучшение системы разбивки документов через AI-группировку смысловых блоков

## Резюме

Проведен анализ текущей системы разбивки документов в ScriptRating и разработана архитектура semantic chunking с использованием AI для группировки смысловых блоков. Предложенное решение обеспечивает значительное улучшение качества поиска и анализа документов при сохранении обратной совместимости.

## Ключевые выводы анализа

### Проблемы текущей системы
1. **Примитивный chunking**: word-based разбивка по фиксированному размеру (1000 символов)
2. **Потеря семантики**: разрыв связей между смысловыми единицами
3. **Игнорирование структуры**: игнорируются заголовки, иерархия, диалоги
4. **Неэффективность для сложных документов**: сценарии разбиваются некорректно

### Анализ документов системы
- **Нормативные критерии**: короткие правовые тексты с ссылками на ФЗ
- **Киносценарии**: сложная структура с сценами, диалогами, временными переходами
- **Пример**: сценарий "Васильки" (3946 строк) содержит 32 сцены, множество персонажей

## Архитектурное решение

### 1. Semantic Chunking Engine
```python
# Новый модуль: app/domain/services/semantic_analyzer.py
class SemanticChunkingEngine:
    async def process_document(self, raw_script: RawScript) -> List[SemanticChunk]:
        # Структурный анализ
        structure = await self.analyze_structure(raw_script)
        
        # AI-группировка по типам документов
        if structure.doc_type == "script":
            return await self.group_script_content(structure)
        elif structure.doc_type == "legal":
            return await self.group_legal_content(structure)
```

### 2. Типы семантических блоков

#### Для киносценариев:
- **Сценарные блоки**: каждая сцена как единая семантическая единица
- **Персонажно-центричные**: все реплики и действия персонажа
- **Тематические**: группировка по сюжетным линиям

#### Для нормативных документов:
- **Статейно-ориентированные**: каждая статья/пункт
- **Концептуальные**: определения + применение + примеры

### 3. AI-модели для обработки

#### Рекомендуемые модели:
- **Структурный анализ**: GPT-4, Claude 3 Opus
- **Embeddings**: `text-embedding-3-large`, `sentence-transformers/all-MiniLM-L6-v2`
- **Для русских текстов**: SberCloud RuBERT, `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`

## Техническая реализация

### 1. Новые компоненты системы

```
app/
├── domain/entities/
│   ├── semantic_chunk.py          # Расширенная модель чанка
│   ├── document_structure.py      # Структурная информация
│   └── entity.py                  # Извлеченные сущности
├── domain/services/
│   ├── semantic_analyzer.py       # Основной сервис
│   ├── embedding_service.py       # Генерация embeddings
│   └── entity_extraction_service.py # Извлечение сущностей
└── infrastructure/ai/
    ├── openai_client.py           # AI API клиенты
    └── semantic_models.py         # Настройки моделей
```

### 2. Расширенные метаданные

```python
@dataclass
class SemanticChunk:
    # Базовая информация
    content: str
    chunk_type: str  # scene, dialogue, action, article
    
    # Структурные метаданные
    document_structure: Dict[str, Any]
    entities: List[Entity]  # персонажи, локации
    
    # AI-генерированные данные
    summary: str
    key_points: List[str]
    topics: List[str]
    importance_score: float
    
    # Связность
    related_chunks: List[str]
    coherence_score: float
```

### 3. Интеграция с существующей системой

#### Совместимость с RAG Document:
```python
class RAGDocumentManager:
    async def process_document(self, file_path: Path) -> List[RAGDocument]:
        # 1. Используем существующий парсер
        raw_script = await self.legacy_parser.parse_document(file_path)
        
        # 2. Применяем semantic chunking
        semantic_chunks = await self.semantic_pipeline.process(raw_script)
        
        # 3. Конвертируем в RAG документы с расширенными метаданными
        return self.convert_to_rag_documents(raw_script, semantic_chunks)
```

#### Обратная совместимость:
```python
class BackwardCompatibleChunker:
    async def get_content_chunks(self, document: RAGDocument, 
                                use_legacy: bool = False) -> List[str]:
        if use_legacy:
            # Fallback к старому методу
            return self.legacy_get_content_chunks(document)
        
        # Semantic chunking
        semantic_chunks = await self.semantic_pipeline.process_by_id(document.id)
        return [chunk.content for chunk in semantic_chunks]
```

## План внедрения

### Этап 1: Базовая инфраструктура (2-3 недели)
1. Создание новых доменных сущностей (`SemanticChunk`, `DocumentStructure`)
2. Реализация структурного анализатора для определения типов документов
3. Интеграция с AI API (OpenAI/Claude)
4. Базовая обратная совместимость

### Этап 2: Semantic Chunking для сценариев (2-3 недели)
1. Парсер структуры киносценариев
2. Группировка по сценам с сохранением диалогов
3. Извлечение персонажей и их связей
4. Тестирование на реальных сценариях

### Этап 3: Semantic Chunking для правовых документов (1-2 недели)
1. Парсер структуры нормативных документов
2. Группировка по статьям и определениям
3. Интеграция с системой критериев рейтинга

### Этап 4: Оптимизация и расширенные функции (2-3 недели)
1. Кэширование embeddings
2. Параллельная обработка
3. Анализ связности между чанками
4. Настройка параметров и A/B тестирование

## Ожидаемые результаты

### Качественные улучшения:
1. **Точность поиска**: +40-60% за счет семантической связанности
2. **Сохранение контекста**: группировка связанной информации в одном блоке
3. **Улучшенная навигация**: структурированные метаданные для быстрого поиска
4. **Богатые аннотации**: AI-генерированные summary и ключевые точки

### Технические преимущества:
1. **Обратная совместимость**: постепенное внедрение без breaking changes
2. **Масштабируемость**: обработка больших документов с сохранением производительности
3. **Гибкость**: настройка под разные типы документов
4. **Расширяемость**: добавление новых AI-функций в будущем

### Метрики успеха:
- Время ответа поиска: < 500ms
- Точность семантического поиска: > 85%
- Удовлетворенность пользователей: > 90%
- Производительность обработки: > 100 документов/час

## Рекомендации

### Немедленные действия:
1. **Начать с этапа 1**: создать базовую инфраструктуру
2. **Провести пилотное тестирование** на 10-20 документах
3. **Настроить мониторинг** производительности и качества

### Долгосрочная стратегия:
1. **Постепенное внедрение** с fallback к legacy методам
2. **A/B тестирование** semantic vs paragraph chunking
3. **Сбор пользовательского фидбэка** для итерационных улучшений
4. **Расширение функций**: анализ тональности, извлечение фактов

### Риски и митигация:
1. **Производительность**: использовать кэширование и параллельную обработку
2. **Стоимость AI API**: мониторинг затрат и оптимизация промптов
3. **Качество разбивки**: continuous testing и настройка параметров
4. **Совместимость**: extensive testing на существующих данных

## Заключение

Предложенная архитектура semantic chunking представляет собой эволюционное улучшение существующей системы разбивки документов. Решение обеспечивает значительное повышение качества поиска и анализа документов при сохранении стабильности и обратной совместимости системы.

Поэтапное внедрение позволит минимизировать риски и получить быструю обратную связь для итерационных улучшений. Ожидаемый ROI от внедрения - значительное улучшение пользовательского опыта и эффективности работы с документами.